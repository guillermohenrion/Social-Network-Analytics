{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "nps tweet ant.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guillermohenrion/Social-Network-Analytics/blob/master/nps_tweet_ant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLolk9MJ_JtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from twython import Twython\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#from PIL import Image\n",
        "from os import path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import ftfy\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim\n",
        "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel\n",
        "import os\n",
        "from gensim.models.wrappers import LdaMallet, ldamallet\n",
        "from gensim.corpora import Dictionary\n",
        "import spacy\n",
        "import pyLDAvis.gensim\n",
        "import csv\n",
        "from unicodedata import normalize\n",
        "import string\n",
        "\n",
        "SEED = 8888\n",
        "#from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "\n",
        "twitter = Twython(\"U63VxPMqLMwTPFUWEcPUeWMng\", \"sbz3lDj50wMdteqmDCXIexM7wuCxapGs89GtOlSDZV5C1W1hQz\",\n",
        "                    \"280807268-MgZfUIUEb1vkPehfjB8sfuicMstiw0Afek49kkNL\", \"g8QammrsFv1zHfWrGtypSkNloEx1tmU2IpzRaW6JJNjIX\")\n",
        "twitter.sleep_on_rate_limit = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBTQFyfc_JtY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sacarURL(t):\n",
        "    p=re.compile('<a [^>]*>')\n",
        "    p2=re.compile('</a>')\n",
        "    r=re.sub(p, '', t)\n",
        "    r=re.sub(p, '', t)\n",
        "    r=re.sub(p, '', t)\n",
        "    r=re.sub(p2, '', r)\n",
        "    r=re.sub(p2, '', r)\n",
        "    r=re.sub(p2, '', r)\n",
        "    return r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRrfFKT-_Jtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################\n",
        "# Funciones\n",
        "######################################################################################################################\n",
        "\n",
        "def cleaner(text, is_topic=False):\n",
        "    if not is_topic:\n",
        "        text = text.lower()  # texto a minúsculas\n",
        "        text = re.sub(r'\\[.*?¿\\]%', ' ', text)  # Se remueven corchetes\n",
        "        text = re.sub(r'\\(.*?¿\\)%', ' ', text)  # Se remueven parentesis\n",
        "        text = re.sub(r'[%s]' % re.escape(string.punctuation), ' ', text)  # Se remueven signos de puntuación\n",
        "        text = re.sub(r'\\w*\\d\\w*', '', text)  # Se remueven palabras que contienen dígitos.\n",
        "        text = re.sub(r'\\n', ' ', text)  # Sustituye nueva línea por un espacio\n",
        "        text = re.sub(r'\\s+', ' ', text, flags=re.I)  # Sustituye los múltiples espacios por sólo un espacio\n",
        "        text = re.sub(r'[‘’“”…«»°*_/¿¡!?]', '', text)  # Remueve otros caracteres no sintácticos\n",
        "        text = re.sub(r\"([^n\\u0300-\\u036f]|n(?!\\u0303(?![\\u0300-\\u036f])))[\\u0300-\\u036f]+|\\u20AC\", r\"\\1\",\n",
        "                      normalize(\"NFD\", text), 0, re.I)  # NFD y eliminar diacríticos y símbolo euro\n",
        "        text = normalize('NFC', text)  # NFC\n",
        "        text_list = []\n",
        "        for w1 in text.split(' '):\n",
        "            text_list.append(w1)\n",
        "        text_non_stop = [word for word in text_list if word not in stopwords.words()]\n",
        "        return text_non_stop\n",
        "    else:\n",
        "        text_non_stop = [word for word in text if word not in stopwords.words()]\n",
        "        bigram_list = bigram[text_non_stop]\n",
        "        out_text = lemmatization(' '.join(bigram_list), allowed_postags=['NOUN'])  # , 'ADJ', 'PROPN'\n",
        "        return out_text\n",
        "\n",
        "\n",
        "def lemmatization(texts, allowed_postags=None):\n",
        "    if allowed_postags is None:\n",
        "        allowed_postags = ['NOUN']\n",
        "    texts_out = [token.lemma_ for token in nlp(texts) if token.pos_ in\n",
        "                 allowed_postags and token.text not in black_list and len(token.text) > 2]\n",
        "    return texts_out\n",
        "\n",
        "\n",
        "def display_topics(model, model_type='lda'):\n",
        "    for topic_idx, topic in enumerate(model.print_topics()):\n",
        "        print('Topic %d:' % topic_idx)\n",
        "        if model_type == 'hdp':\n",
        "            print(\" \".join(re.findall(r'\\*(.[^\\*-S]+).?', topic[1])), '\\n')\n",
        "        else:\n",
        "            print(\" \".join(re.findall(r'\\\"(.[^\"]+).?', topic[1])), '\\n')\n",
        "\n",
        "\n",
        "def evaluate_graph(dictionary, corpus, texts, limit, model):\n",
        "    \"\"\"\n",
        "    Function to display num_topics - LDA graph using c_v coherence\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    limit : topic limit\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    lm_list : List of LDA topic models\n",
        "    c_v : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    c_v = []\n",
        "    lm_list = []\n",
        "    for num_topics in range(1, limit):\n",
        "        if model == 'lsi':\n",
        "            lm = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        elif model == 'mallet':\n",
        "            lm = LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        else:\n",
        "            lm = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
        "        lm_list.append(lm)\n",
        "        cm = CoherenceModel(model=lm, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        c_v.append(cm.get_coherence())\n",
        "\n",
        "    # Show graph\n",
        "    x = range(1, limit)\n",
        "    plt.plot(x, c_v)\n",
        "    plt.xlabel('num_topics')\n",
        "    plt.ylabel('Coherence score')\n",
        "    plt.legend('c_v', loc='best')\n",
        "    plt.show()\n",
        "\n",
        "    return lm_list, c_v\n",
        "\n",
        "\n",
        "def format_topics_sentences(ldamodel=0, corpus=[], texts=0):\n",
        "    sent_topics_df = pd.DataFrame()  # - n\n",
        "\n",
        "    # Tema principal en cada documento\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Tema dominante, porcentaje de contribución y palabras claves\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # Tema dominante\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(\n",
        "                    pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Agrega el texto original al final\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return sent_topics_df\n",
        "\n",
        "\n",
        "######################################################################################################################\n",
        "# Carga de archivos\n",
        "######################################################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGuqRmBO_Jtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "black_list = ['n', 'q', 'eh', 'me', 'xq', 'z', 'x', 'etc', 'd', 'm', 's', 'u', 'p', 'l', 'as', 'av', 'j', 'porq', 'c',\n",
        "              'v', 'cdo', 'i', 'unas', 'k', 't', 'pq', 'b', 'g', 'ne', 'r', 'h', 'qu', 'f', 'co', 'ud', 'sta', 'srta',\n",
        "              'sr', 'sra', 'srita', 'tb', 'osea', 'po', 'ala', 'ami', 'ka', 'mo', 'migo', 'lis', 'in', 'idas', 'asique',\n",
        "              'aunq', 'miy', 'lla', 'unlam', 'na', 'don', 'xxxx', 'yla', 'qie', 'qur', 'qye', 'fu', 'my', 'ke', 'laa',\n",
        "              'll', 'mm', 'tmb', 'pr', 'ay', 'or', 'nc', 'xk', 'xx', 'ye', 'rn', 'hr', 'eramos', 'estabamos', 'estais',\n",
        "              'estan', 'estara', 'estaran', 'estaras', 'estare', 'estareis', 'estaria', 'estariais', 'estariamos',\n",
        "              'estarian', 'estarias', 'esteis', 'esten', 'estes', 'estuvieramos', 'estuviesemos', 'fueramos',\n",
        "              'fuesemos', 'habeis', 'habia', 'habiais', 'habiamos', 'habian', 'habias', 'habra', 'habran', 'habras',\n",
        "              'habre', 'habreis', 'habria', 'habriais', 'habriamos', 'habrian', 'habrias', 'hayais', 'hubieramos',\n",
        "              'hubiesemos', 'mas', 'mia', 'mias', 'mio', 'mios', 'seais', 'sera', 'seran', 'seras', 'sere', 'sereis',\n",
        "              'seria', 'seriais', 'seriamos', 'serian', 'serias', 'si', 'tambien', 'tendra', 'tendran', 'tendras',\n",
        "              'tendre', 'tendreis', 'tendria', 'tendriais', 'tendriamos', 'tendrian', 'tendrias', 'teneis', 'tengais',\n",
        "              'tenia', 'teniais', 'teniamos', 'tenian', 'tenias', 'tuvieramos', 'tuviesemos', 'vez']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qorWHEC7_Jtu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#stop = set(stopwords.words('spanish'))\n",
        "#additional_stopwords = set(black_list)\n",
        "#stopwords = stop.union(additional_stopwords)\n",
        "\n",
        "nlp = spacy.load('es_core_news_sm')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2a5gTHG_Jt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_tweets = twitter.get_user_timeline(screen_name='alferdez',\n",
        "                                        include_rts=True, count=200, tweet_mode=\"extended\")\n",
        "tweetdf=pd.DataFrame([], columns=['id', 'text'])\n",
        "for tweet in user_tweets:\n",
        "    tweet['text'] = Twython.html_for_tweet(tweet)\n",
        "    new_row={'id':tweet['id'], 'text':sacarURL(tweet['text'])}\n",
        "    tweetdf=tweetdf.append(new_row, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMRaf5YW_Jt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweetdf['text_orig'] = tweetdf['text']  # Resguardamos original\n",
        "tweetdf['text'] = tweetdf['text'].apply(cleaner, args=(False,))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtmAqVQh_JuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bigram = gensim.models.Phrases(tweetdf.text.to_list(), min_count=5, threshold=10.0)\n",
        "tweetdf['text'] = tweetdf['text'].apply(cleaner, args=(True,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVwUYluq_JuF",
        "colab_type": "code",
        "colab": {},
        "outputId": "efb0851f-6e32-4934-9b86-ead6da779b41"
      },
      "source": [
        "tweetdf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>text_orig</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1273464435684900864</td>\n",
              "      <td>[medir, cantidad, gente, lugar, motivar, creci...</td>\n",
              "      <td>La medida apunta a reducir la cantidad de gent...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1273464433440952325</td>\n",
              "      <td>[crecimiento, contagio, partir, viernes, trans...</td>\n",
              "      <td>Tras una reunión con @Kicillofok y @diegosanti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1273462599313756160</td>\n",
              "      <td>[anunciar, transportar, trabajador, medir, aut...</td>\n",
              "      <td>IMPORTANTE‼️ @alferdez  anunció que desde maña...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1273460113114791936</td>\n",
              "      <td>[problema, diferenciar, suffix_twitter]</td>\n",
              "      <td>Esta noche el Presidente @alferdez nos convocó...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1273459546783129605</td>\n",
              "      <td>[salud, coser, vida, muerto, suffix_twitter]</td>\n",
              "      <td>\"No tengo el dilema entre la salud y la econom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>1263828658017439745</td>\n",
              "      <td>[moldavskyrober, abrazar, filmina]</td>\n",
              "      <td>Jajaja me has hecho reír mucho, querido @molda...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>1263777085627252739</td>\n",
              "      <td>[empezo, pyme, hecho, empresa, programar, empl...</td>\n",
              "      <td>Ni bien empezó la pandemia el presidente @alfe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>1263637108046127104</td>\n",
              "      <td>[compromiso, encerrar, suffix_twitter]</td>\n",
              "      <td>No vamos a asumir ningún compromiso con nuestr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>1263624086179151872</td>\n",
              "      <td>[provincia, actividad, protocolo, severo, resp...</td>\n",
              "      <td>También visité Tucumán, una provincia que se m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>1263622846607249414</td>\n",
              "      <td>[empresa, protocolar, esforzar, nyjtkwlcgf]</td>\n",
              "      <td>En Santiago del Estero visité Mega Alfalfa Arg...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id                                               text  \\\n",
              "0    1273464435684900864  [medir, cantidad, gente, lugar, motivar, creci...   \n",
              "1    1273464433440952325  [crecimiento, contagio, partir, viernes, trans...   \n",
              "2    1273462599313756160  [anunciar, transportar, trabajador, medir, aut...   \n",
              "3    1273460113114791936            [problema, diferenciar, suffix_twitter]   \n",
              "4    1273459546783129605       [salud, coser, vida, muerto, suffix_twitter]   \n",
              "..                   ...                                                ...   \n",
              "195  1263828658017439745                 [moldavskyrober, abrazar, filmina]   \n",
              "196  1263777085627252739  [empezo, pyme, hecho, empresa, programar, empl...   \n",
              "197  1263637108046127104             [compromiso, encerrar, suffix_twitter]   \n",
              "198  1263624086179151872  [provincia, actividad, protocolo, severo, resp...   \n",
              "199  1263622846607249414        [empresa, protocolar, esforzar, nyjtkwlcgf]   \n",
              "\n",
              "                                             text_orig  \n",
              "0    La medida apunta a reducir la cantidad de gent...  \n",
              "1    Tras una reunión con @Kicillofok y @diegosanti...  \n",
              "2    IMPORTANTE‼️ @alferdez  anunció que desde maña...  \n",
              "3    Esta noche el Presidente @alferdez nos convocó...  \n",
              "4    \"No tengo el dilema entre la salud y la econom...  \n",
              "..                                                 ...  \n",
              "195  Jajaja me has hecho reír mucho, querido @molda...  \n",
              "196  Ni bien empezó la pandemia el presidente @alfe...  \n",
              "197  No vamos a asumir ningún compromiso con nuestr...  \n",
              "198  También visité Tucumán, una provincia que se m...  \n",
              "199  En Santiago del Estero visité Mega Alfalfa Arg...  \n",
              "\n",
              "[200 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHb8lQWU_JuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Corpus y diccionario para gensim\n",
        "dictionary = Dictionary(tweetdf['text'].to_list())\n",
        "dictionary.compactify()\n",
        "# Filtrado de extremos\n",
        "dictionary.filter_extremes(no_below=2, no_above=0.97, keep_n=None)\n",
        "dictionary.compactify()\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in tweetdf['text'].to_list()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YhceV_b_JuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lmlist_lsi, c_v = evaluate_graph(dictionary=dictionary, corpus=corpus, texts=tweetdf['text'].to_list(),\n",
        "                                 limit=50, model='lsi')  # 10, 30?\n",
        "\n",
        "\n",
        "ldamodel = LdaModel(corpus=corpus, num_topics=5, id2word=dictionary)\n",
        "display_topics(ldamodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nj8QF-v_JuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lsimodel = LsiModel(corpus=corpus, num_topics=5, id2word=dictionary)\n",
        "display_topics(lsimodel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHytE9XX_JuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(lmlist_lsi)\n",
        "print(c_v)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}